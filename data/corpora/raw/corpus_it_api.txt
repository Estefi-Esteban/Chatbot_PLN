L'elaborazione del linguaggio naturale (NLP, da Natural Language Processing) √® una sottobranca della linguistica computazionale e intelligenza artificiale che tratta l'interazione tra i computer e il linguaggio umano, in particolare sul come programmare i computer per elaborare e analizzare grandi quantit√† di dati di linguaggio naturale. Lo scopo √® rendere una macchina in grado di "comprendere" il contenuto dei documenti e le loro sfumature contestuali, in modo tale che possa quindi estrarre con precisione informazioni e idee contenute nei documenti, nonch√© classificare e categorizzare i documenti stessi.
Le sfide dell'elaborazione del linguaggio coinvolgono spesso il riconoscimento vocale, comprensione del linguaggio naturale e la generazione del linguaggio naturale.
Questo processo √® reso particolarmente difficile e complesso a causa delle caratteristiche intrinseche di ambiguit√† del linguaggio umano. Per questo motivo il processo di elaborazione viene suddiviso in fasi diverse, tuttavia simili a quelle che si possono incontrare nel processo di elaborazione di un linguaggio di programmazione:
analisi lessicale: scomposizione di un'espressione linguistica in token (in questo caso le parole)
analisi grammaticale: associazione delle parti del discorso a ciascuna parola nel testo
analisi sintattica: arrangiamento dei token in una struttura sintattica, conosciuta come albero sintattico
analisi semantica: assegnazione di un significato (semantica) alla struttura sintattica e, di conseguenza, all'espressione linguistica
Nell'analisi semantica la procedura automatica che attribuisce all'espressione linguistica un significato tra i diversi possibili √® detta disambiguazione.
In teoria, l'elaborazione del linguaggio naturale √® un metodo di interazione uomo-macchina. I primi sistemi sviluppati, quali SHRDLU, che lavoravano in "mondi a blocchi" con vocabolari ristretti, ottenevano ottimi risultati. Ci√≤ port√≤ i ricercatori a un eccessivo ottimismo, che scem√≤ non appena i sistemi furono estesi a situazioni pi√π realistiche con problemi reali di ambiguit√† e complessit√†.
La comprensione del linguaggio naturale √® spesso considerata un problema IA-completo, poich√© si pensa che il riconoscimento del linguaggio richieda una conoscenza estesa del mondo e una grande capacit√† di manipolarlo. Per questa ragione, la definizione di "comprensione" √® uno dei maggiori problemi dell'elaborazione del linguaggio naturale.
Primi approcci (Simbolico e Statistico)
Anni '50 e '60: Prime Teorie e Approcci Simbolici
Inizialmente, l'elaborazione del linguaggio naturale si basava principalmente su approcci simbolici e basati su regole. Questi approcci cercavano di analizzare la sintassi e la semantica del linguaggio utilizzando regole grammaticali. Alcuni dei primi sistemi, come quelli di Alan Turing e Noam Chomsky, cercavano di modellare la struttura linguistica attraverso grammatica formale e logica.
Gli approcci statistici al NLP sono nati in risposta alla difficolt√† di trattare il linguaggio naturale in modo rigoroso e deterministico. Il linguaggio umano √® complesso, ambiguo e variabile, quindi gli approcci simbolici basati su regole (come quelli usati negli anni '50 e '60) non erano sempre sufficienti. Tra la fine degli anni '80 e la met√† degli anni '90 del millenovecento, l'approccio statistico ha posto fine al periodo invernale dell'intelligenza artificiale, causato dall'inefficacia degli approcci basati sulle regole.
Probabilit√† e Modelli Statistici
I primi approcci statistici, capaci di superare i precedentemente diffusi alberi decisionali, che creano sistemi di regole rigide "se-allora", erano basati su modelli probabilistici, come i modelli di Markov e le catene di Markov nascoste (HMM). Questi modelli sono stati utilizzati principalmente per il part-of-speech tagging, l'analisi sintattica e il riconoscimento del parlato. Gli HMM sono modelli probabilistici che descrivono sequenze di eventi (o stati) in cui la probabilit√† di transizione da uno stato all'altro dipende solo dallo stato attuale.
Altri modelli di probabilit√† ampiamente utilizzati includevano i modelli n-grammi per la previsione delle parole (un modello di linguaggio che stima la probabilit√† di una parola basata sulle precedenti n‚àí1 parole).
Probabilit√† condizionata nei modelli di linguaggio probabilistici
La probabilit√† condizionata √® un concetto fondamentale nella teoria delle probabilit√† e viene applicato in numerosi ambiti, tra cui i modelli di linguaggio probabilistici. In generale, la probabilit√† condizionata di un evento ùë§·µ¢, dato un insieme di eventi precedenti
{\displaystyle w_{1},w_{2},\ldots ,w_{i-1}}
{\displaystyle P(w_{i}\mid w_{1},w_{2},\ldots ,w_{i-1})}
In altre parole, si cerca di calcolare la probabilit√† che una determinata parola (ùë§·µ¢) appaia, dato che sono gi√† state osservate le parole precedenti nel contesto (
{\displaystyle w_{1},w_{2},\ldots ,w_{i-1}}
). Questo tipo di probabilit√† √® fondamentale nella comprensione del linguaggio naturale, in quanto i modelli linguistici devono prevedere la probabilit√† di una parola in un dato contesto.
Modelli di linguaggio probabilistici
Nei modelli di linguaggio probabilistici, la probabilit√† di una parola dipende dalle parole che l'hanno preceduta. Questo approccio √® alla base di molti sistemi di trattamento del linguaggio naturale, come i sistemi di correttore ortografico, i motori di ricerca e le traduzioni automatiche.
Un modello linguistico probabilistico pu√≤ essere basato su diversi ordini di dipendenza, tra cui il modello n-gramma. In un modello n-gramma, la probabilit√† condizionata di una parola ùë§·µ¢ viene approssimata considerando solo le parole precedenti in una finestra di dimensione fissa, ovvero le ultime ùëõ-1 parole. La probabilit√† condizionata di ùë§·µ¢, data una sequenza di parole precedenti, √® quindi approssimata come:
{\displaystyle P(w_{i}\mid w_{1},w_{2},\ldots ,w_{i-1})\approx P(w_{i}\mid w_{i-n+1},w_{i-n+2},\ldots ,w_{i-1})}
Questa approssimazione riduce il problema complesso di calcolare la probabilit√† condizionata di una parola data l'intera sequenza di parole precedenti, limitandosi a considerare solo le ultime ùëõ-1 parole, e semplifica il calcolo, rendendo il modello computazionalmente gestibile.
Ulteriori esempi di modello n-gramma
Un esempio di modello n-gramma di ordine 2 (bigramma) considera solo la parola precedente per calcolare la probabilit√† di una nuova parola. In altre parole, la probabilit√† condizionata di una parola ùë§·µ¢ data la sequenza precedente di parole ùë§‚ÇÅ, ùë§‚ÇÇ, ..., ùë§·µ¢‚Çã‚ÇÅ √® approssimata come:
{\displaystyle P(w_{i}\mid w_{1},w_{2},\ldots ,w_{i-1})\approx P(w_{i}\mid w_{i-1})}
Nel caso di un modello trigramma (ordine 3), invece, si considera una finestra di 2 parole precedenti per calcolare la probabilit√† della parola successiva:
{\displaystyle P(w_{i}\mid w_{1},w_{2},\ldots ,w_{i-1})\approx P(w_{i}\mid w_{i-2},w_{i-1})}
Questa approccio riduce significativamente la complessit√† rispetto al calcolo di una probabilit√† condizionata su tutte le parole precedenti, ma richiede comunque una buona quantit√† di dati per stimare accuratamente le probabilit√†.
Limitazioni dei modelli n-gramma
Nonostante i modelli n-gramma siano semplici e computazionalmente efficienti, presentano alcune limitazioni. La principale √® che questi modelli non riescono a catturare dipendenze a lungo termine tra le parole. Ad esempio, in una frase come Il cane corre nel parco e poi si ferma per riposare, un modello di ordine n=2 (bigramma) potrebbe non essere in grado di cogliere correttamente la relazione tra corre e ferma, poich√© queste parole sono distanti.
Inoltre, per modelli di ordine pi√π alto (ad esempio, trigramma o quadgramma), il numero di parametri aumenta esponenzialmente, il che porta alla necessit√† di una grande quantit√† di dati per stimare accuratamente le probabilit√†, oltre al rischio di sparsit√† dei dati (mancanza di alcune combinazioni di parole nei dati di addestramento).
Un processo di Markov √® un modello probabilistico che descrive un sistema in cui la probabilit√† di passare a uno stato futuro dipende esclusivamente dallo stato attuale, e non dalla sequenza di eventi che ha portato a quello stato. Questo principio √® noto come propriet√† di Markov o memoria corta. In altre parole, un processo di Markov √® un tipo di processo stocastico in cui il futuro √® indipendente dal passato, dato l'istante presente.
Nel contesto del linguaggio naturale, un modello di Markov pu√≤ essere utilizzato per prevedere la probabilit√† di una parola successiva basandosi solo sulla parola precedente. Ad esempio, in un modello di Markov di ordine 1 (noto anche come bigramma), la probabilit√† di una parola dipende solo dalla parola che la precede, ed √® rappresentata come:
{\displaystyle P(w_{n}\mid w_{n-1},w_{1},w_{2},\ldots ,w_{n-1})=P(w_{n})}
Questo approccio semplifica il calcolo delle probabilit√†, riducendo la dipendenza dalla sequenza completa di parole precedenti a quella di una sola parola, ma pu√≤ perdere informazioni contestuali importanti che si trovano in sequenze pi√π lunghe.
Catene di Markov Nascoste (HMM)
Un modello di Markov a catene nascoste (Hidden Markov Model, HMM) √® un'estensione del modello di Markov che coinvolge due sequenze: una visibile e una nascosta. In un HMM, gli stati nascosti non sono direttamente osservabili (ad esempio, i tag grammaticali), ma influenzano la sequenza osservata (come le parole in un testo).
L'obiettivo principale degli HMM √® inferire la sequenza di stati nascosti basandosi sui dati osservati, come le parole. Ad esempio, in un'applicazione di analisi del testo o part-of-speech tagging, un HMM pu√≤ essere utilizzato per identificare le categorie grammaticali delle parole, dato il loro contesto. La probabilit√† di una sequenza di parole, quindi, dipende dalla sequenza di stati nascosti associata ad esse.
La formula di base di un modello a catene di Markov nascoste √® la seguente:
{\displaystyle (S_{t-1})\,P(W^{t}\mid S^{t})\,P(W,S)=P(S_{1})\prod _{t=2}^{T}P(S^{t})}
ùëä √® la sequenza di parole osservabili (ad esempio, "il cane corre"),
ùëÜ √® la sequenza di stati nascosti (ad esempio, i tag grammaticali come "Aggettivo", "Sostantivo", "Verbo"),
P(ùëÜ·µó | ùëÜ·µó‚Çã‚ÇÅ) √® la probabilit√† di transizione tra gli stati nascosti,
P(ùëä·µó | ùëÜ·µó) √® la probabilit√† di osservare la parola ùëä·µó dato lo stato nascosto ùëÜ·µó.
La probabilit√† di una sequenza di parole osservate e la sequenza di stati nascosti √® calcolata come il prodotto della probabilit√† iniziale del primo stato nascosto, la probabilit√† di transizione tra gli stati e la probabilit√† di osservare la parola in base allo stato nascosto. Gli HMM sono ampiamente utilizzati per compiti di riconoscimento vocale, traduzione automatica, e analisi del linguaggio naturale.
Applicazioni delle catene di Markov nascoste
Gli HMM sono utilizzati in vari ambiti dell'intelligenza artificiale e del trattamento del linguaggio naturale, tra cui:
Part-of-Speech Tagging: assegnare un tag grammaticale a ciascuna parola in una frase.
Riconoscimento vocale: convertire la voce in testo, utilizzando modelli che prevedono la sequenza di fonemi nascosti.
Analisi del sentimento: determinare il tono o il sentimento di un testo a partire da un modello di stati emotivi nascosti.
Bioinformatica: per esempio, nella predizione della struttura secondaria delle proteine.
Nonostante la loro utilit√†, gli HMM sono limitati dalla loro dipendenza da sequenze di stati nascosti e dal fatto che non catturano efficacemente dipendenze a lungo termine, un problema che √® stato affrontato con l'introduzione di modelli pi√π avanzati come le reti neurali ricorrenti (RNN) e i modelli basati su Transformer.
Anni 2000 e 2010 - L'avvento dell'IA
Il passaggio dai modelli statistici tradizionali ai modelli di intelligenza artificiale (IA) nel campo dell'elaborazione del linguaggio naturale (NLP) √® stato un cambiamento fondamentale nella tecnologia del linguaggio. Questo cambiamento √® stato guidato da ricercatori e sviluppatori come Geoffrey Hinton, Yann LeCun e Yoshua Bengio, che hanno contribuito allo sviluppo delle reti neurali profonde (deep learning). Negli anni 2000 e 2010, le tecniche basate su modelli statistici, come i modelli di Markov o gli n-grammi, dimostravano di essere limitate nelle loro capacit√† di gestire dipendenze a lungo termine e dalla necessit√† di grandi quantit√† di dati etichettati per funzionare correttamente.
Con l'avvento delle reti neurali, in particolare delle reti neurali ricorrenti (RNN) e pi√π recentemente delle architetture Transformer (come quelle utilizzate in modelli come GPT e BERT), √® stato possibile affrontare questi limiti. Le reti neurali sono in grado di apprendere rappresentazioni pi√π complesse e dinamiche del linguaggio, catturando meglio le relazioni semantiche e sintattiche a lungo termine tra le parole. Inoltre, l'accesso a enormi quantit√† di dati e l'aumento della potenza computazionale hanno permesso l'allenamento di modelli molto pi√π sofisticati e precisi.
Il passaggio ai modelli di IA √® stato anche alimentato dalla crescente disponibilit√† di GPU e hardware specializzato, che hanno permesso di ridurre i tempi di addestramento e migliorare le prestazioni. Le tecniche di deep learning hanno dimostrato di superare significativamente i modelli statistici in compiti complessi, come la traduzione automatica, il riconoscimento del parlato e la comprensione del linguaggio naturale, portando a una rivoluzione nell'automazione linguistica e nell'intelligenza artificiale. Questo cambiamento ha trasformato l'NLP da una disciplina basata su regole e probabilit√† a un campo guidato dalla capacit√† delle macchine di "imparare" autonomamente dalle enormi quantit√† di dati a loro disposizione.
Reti neurali ricorrenti (RNN)
Le reti neurali ricorrenti (RNN) sono state introdotte negli anni '80 come modello innovativo per gestire dati sequenziali, rappresentando un importante progresso nell'elaborazione del linguaggio naturale (NLP). Grazie alla loro capacit√† di mantenere una memoria temporale, le RNN sono state utilizzate per attivit√† come traduzione automatica, analisi del sentimento e modellazione del linguaggio. Tuttavia, i loro limiti nel catturare dipendenze a lungo termine hanno portato allo sviluppo di varianti pi√π avanzate, come LSTM e GRU, che hanno migliorato significativamente le performance in NLP.
Le reti neurali ricorrenti (RNN) sono progettate per elaborare sequenze di dati utilizzando una struttura ricorsiva che consente di mantenere una memoria dei passi precedenti. A ogni passo temporale, l'RNN aggiorna il suo stato nascosto in base all'input corrente e allo stato nascosto precedente.
La formula base √® la seguente:
{\displaystyle h_{t}=\sigma (W_{h}h_{t-1}+W_{x}x_{t}+b)}
{\displaystyle W_{h}}
{\displaystyle W_{x}}
sono matrici di peso,
√® un vettore di bias,
{\displaystyle \sigma }
√® una funzione di attivazione, come la tangente iperbolica o la ReLU.
L'output pu√≤ essere calcolato come:
{\displaystyle y_{t}=\sigma _{o}(W_{y}h_{t}+b_{y})}
Questa struttura consente alle RNN di "ricordare" informazioni lungo una sequenza, il che le rende valide opzioni in vari campi, come (appunto), l'elaborazione del linguaggio naturale. Tuttavia, problemi come il vanishing gradient limitano la loro capacit√† di apprendere dipendenze a lungo termine.
Varianti delle Reti Neurali Ricorrenti: LSTM e GRU
Le limitazioni delle RNN tradizionali, come l'incapacit√† di catturare efficacemente le dipendenze a lungo termine a causa del problema del vanishing gradient, hanno portato allo sviluppo di architetture pi√π avanzate. Tra queste, le Long Short-Term Memory (LSTM) e le Gated Recurrent Unit (GRU) si sono dimostrate particolarmente efficaci in una vasta gamma di applicazioni.
Long Short-Term Memory (LSTM)
Le LSTM sono state introdotte per affrontare i problemi delle RNN standard. Il loro design include meccanismi specifici, chiamati gates, che regolano il flusso di informazioni nella rete, consentendo di preservare informazioni rilevanti per lunghi intervalli temporali. La struttura principale di un'unit√† LSTM include:
Forget Gate: Decide quali informazioni dello stato precedente devono essere scartate.
{\displaystyle f_{t}=\sigma (W_{f}\cdot [h_{t-1},x_{t}]+b_{f})}
Input Gate: Determina quali nuove informazioni devono essere aggiunte allo stato della memoria.
{\displaystyle i_{t}=\sigma (W_{i}\cdot [h_{t-1},x_{t}]+b_{i})}
{\displaystyle {\tilde {C}}t=\tanh(W_{c}\cdot [h{t-1},x_{t}]+b_{c})}
Cell State Update: Aggiorna lo stato della memoria combinando le informazioni filtrate dai gate.
{\displaystyle C_{t}=f_{t}\cdot C_{t-1}+i_{t}\cdot {\tilde {C}}_{t}}
Output Gate: Determina l'output basato sullo stato aggiornato della cella.
{\displaystyle o_{t}=\sigma (W_{o}\cdot [h_{t-1},x_{t}]+b_{o})}
{\displaystyle h_{t}=o_{t}\cdot \tanh(C_{t})}
Gated Recurrent Unit (GRU)
Le GRU sono un'altra variante delle RNN progettata per migliorare l'efficienza e ridurre la complessit√† delle LSTM. La principale differenza tra GRU e LSTM √® l'assenza di uno stato della memoria separato: nelle GRU, lo stato nascosto combina sia la memoria che il controllo del flusso di informazioni. Le GRU utilizzano due gates principali:
Update Gate: Regola quanto dello stato precedente deve essere mantenuto.
{\displaystyle z_{t}=\sigma (W_{z}\cdot [h_{t-1},x_{t}]+b_{z})}
Reset Gate: Decide quanto del passato deve essere "dimenticato" nel calcolo dello stato candidato.
{\displaystyle r_{t}=\sigma (W_{r}\cdot [h_{t-1},x_{t}]+b_{r})}
Stato Candidato e Output:
{\displaystyle {\tilde {h}}t=\tanh(W_{h}\cdot [r_{t}\cdot h{t-1},x_{t}]+b_{h})}
{\displaystyle h_{t}=z_{t}\cdot h_{t-1}+(1-z_{t})\cdot {\tilde {h}}_{t}}
Le GRU, essendo pi√π semplici, richiedono meno risorse computazionali rispetto alle LSTM, rendendole adatte per applicazioni in cui la velocit√† √® cruciale, senza compromettere troppo le performance.
Transformer: Un'innovazione nelle reti neurali sequenziali
Con l'obiettivo di superare alcune limitazioni delle architetture ricorrenti come le RNN, LSTM e GRU, i Transformer sono stati introdotti da Vaswani et al. nel 2017. Questa architettura ha rivoluzionato il campo dell'elaborazione del linguaggio naturale e oltre, eliminando la dipendenza dalla computazione sequenziale tipica delle RNN e introducendo meccanismi di attenzione altamente efficaci.
Architettura dei Transformer
L'architettura del Transformer √® composta da due blocchi principali: un encoder e un decoder, ciascuno dei quali utilizza meccanismi di attenzione e feedforward per elaborare le sequenze. La chiave del successo dei Transformer risiede nel loro utilizzo del meccanismo di attenzione multi-testa e nella capacit√† di elaborare le sequenze in parallelo.
Codificatore (encoder)
Il blocco encoder del Transformer √® costituito da una sequenza di sotto-strati:
Meccanismo di Attenzione Multi-Testa: Consente al modello di focalizzarsi su parti diverse della sequenza di input simultaneamente.Ogni testa calcola:
{\displaystyle {\text{Attenzione}}(Q,K,V)={\text{softmax}}\left({\frac {QK^{T}}{\sqrt {d_{k}}}}\right)V}
Rete Feedforward: Dopo l'attenzione, i dati passano attraverso una rete completamente connessa con attivazione non lineare.
Norma e Residuo: Ogni sotto-strato √® seguito da una connessione residua e una normalizzazione batch, migliorando la stabilit√† del training.
Decodificatore (decoder)
Il decoder √® simile all'encoder, ma con un'aggiunta chiave: il meccanismo di attenzione incrociata. Questo permette al decoder di focalizzarsi sull'output dell'encoder durante la generazione della sequenza.
Attenzione Mascherata: Utilizzata per garantire che il decoder non possa "vedere" token futuri durante la generazione di una sequenza.
Attenzione Incrociata: Consente al decoder di combinare l'informazione del contesto (encoder) con i dati gi√† generati.
Embedding e Codifica Posizionale
Poich√© il Transformer non elabora sequenze in ordine, utilizza un encoding posizionale per fornire informazioni sulla posizione relativa dei token. Questo viene aggiunto ai vettori embedding dell'input:
{\displaystyle PE_{(pos,2i)}=\sin \left({\frac {pos}{10000^{2i/d_{\text{model}}}}}\right)}
{\displaystyle PE_{(pos,2i+1)}=\cos \left({\frac {pos}{10000^{2i/d_{\text{model}}}}}\right)]}
Il cuore del Transformer √® il meccanismo di self-attention, che valuta l'importanza di ciascun token rispetto agli altri nella sequenza. Questo approccio elimina la dipendenza temporale lineare delle RNN, rendendo il modello altamente parallelo e pi√π efficiente in termini di calcolo.
Applicazioni dei Transformer
Dopo il successo iniziale del Transformer, questa architettura √® diventata la base per una vasta gamma di modelli avanzati nel campo dell'intelligenza artificiale. Tra i pi√π significativi vi √® BERT (Bidirectional Encoder Representations from Transformers), un modello pre-addestrato che sfrutta una rappresentazione bidirezionale per comprendere il contesto di una parola analizzando sia il testo precedente che quello successivo, risultando particolarmente efficace in compiti come il completamento di frasi e l'analisi del sentimento. Un altro modello influente √® GPT (Generative Pre-trained Transformer), che adotta un approccio unidirezionale focalizzato sulla generazione di testo, eccellendo nella creazione di contenuti coerenti e realistici, oltre a una vasta gamma di attivit√† linguistiche. T5 (Text-to-Text Transfer Transformer) propone un approccio generalizzato in cui tutti i compiti di elaborazione del linguaggio naturale sono formulati come trasformazioni di input e output testuali, rendendo il modello estremamente versatile. Inoltre, l'architettura Transformer √® stata adattata anche al dominio della visione artificiale con il Vision Transformer (ViT), progettato per attivit√† come la classificazione delle immagini, dimostrando che i Transformer possono essere efficaci anche in compiti che tradizionalmente erano dominati da reti convoluzionali.
Confronto con le Architetture Ricorrenti
I Transformer hanno rivoluzionato settori come la traduzione automatica, il riassunto automatico e l'analisi del testo, rendendo obsolete molte architetture ricorrenti grazie a diverse caratteristiche chiave. Tra queste, l'efficienza computazionale, che consente di elaborare intere sequenze in parallelo, superando i limiti delle RNN, che processano i dati in modo sequenziale. Un altro vantaggio fondamentale √® la maggiore capacit√† di catturare contesti lunghi, superando le difficolt√† del vanishing gradient che affliggevano le reti ricorrenti, permettendo ai Transformer di mantenere informazioni rilevanti anche in sequenze molto estese. Infine, la loro scalabilit√† li rende facilmente adattabili a modelli di grandi dimensioni, consentendo un'espansione efficace per affrontare compiti sempre pi√π complessi e diversificati.
Il BOOM degli LLM (2020 e oltre)
Lo sviluppo e gli investimenti nei large language models (LLMs) dal 2020 al 2024 rappresentano un periodo senza precedenti di innovazione nell'intelligenza artificiale. Questi progressi sono stati alimentati da miglioramenti nell'hardware, dalla disponibilit√† di enormi quantit√† di dati online e da innovazioni nelle tecniche di apprendimento automatico, come il transfer learning e l'apprendimento auto-supervisionato. Gli LLM hanno trasformato profondamente vari settori industriali.
Gli investimenti privati in LLM hanno inizialmente registrato una crescita esplosiva, raggiungendo i 189,6 miliardi di dollari nel 2021. Tuttavia, nel 2022 si √® verificata una lieve contrazione, in parte dovuta al rallentamento economico globale. Al contrario, i finanziamenti pubblici per la ricerca sull'IA sono aumentati, specialmente negli Stati Uniti, dove le agenzie governative non legate alla difesa hanno allocato 1,7 miliardi di dollari per la ricerca e lo sviluppo di IA nello stesso anno.
La competizione tra i giganti tecnologici, come OpenAI, Google, Meta e Microsoft, si √® intensificata. OpenAI, con ChatGPT, ha reso gli LLM accessibili al grande pubblico, mentre Meta ha promosso modelli open-source come LLaMA, e Google ha spinto l'innovazione con modelli come Gemini. Anche i modelli pi√π piccoli e specializzati, come PHI-2 di Microsoft, hanno dimostrato che strategie di scala mirate possono competere con modelli pi√π grandi, favorendo una diversificazione nelle dimensioni e negli utilizzi delle reti.
Gli LLM sono stati ampiamente adottati in settori come la sanit√†, la finanza e l'istruzione, migliorando i servizi personalizzati e i processi decisionali. La loro integrazione nei prodotti di consumo ha trasformato interi mercati, portando a una crescente domanda di sistemi multilingue ed etici.
Nel 2024, il mercato globale degli LLM ha continuato a crescere, con il Nord America e la regione Asia-Pacifico in testa per innovazione e adozione.
ChatGPT, DeepSeek e lo scontro tra USA e Cina
Nel gennaio 2025, la startup cinese DeepSeek ha introdotto il modello di intelligenza artificiale DeepSeek-R1, progettato per compiti complessi come il ragionamento matematico e la programmazione. Questo modello open source ha attirato l'attenzione per le sue prestazioni avanzate e per l'efficienza nel consumo di risorse computazionali. La sua capacit√† di combinare ragionamento esplicito e ricerca web in tempo reale ha messo in discussione il predominio statunitense nel settore dell'IA. Tuttavia, il successo di DeepSeek ha sollevato preoccupazioni riguardo a possibili violazioni della propriet√† intellettuale, con OpenAI e Microsoft che hanno accusato la startup cinese di aver utilizzato i loro modelli proprietari per addestrare le proprie applicazioni.
DeepSeek ha sviluppato il suo modello di intelligenza artificiale con un budget di circa 5,6 milioni di dollari, utilizzando 2.048 GPU Nvidia H800 per un totale di 2,788 milioni di ore GPU. In confronto, si stima che OpenAI abbia investito oltre 100 milioni di dollari per l'addestramento di GPT-4, impiegando pi√π di 20.000 GPU Nvidia H100. Questa differenza evidenzia l'efficienza di DeepSeek nell'uso delle risorse computazionali.
Una delle caratteristiche pi√π rivoluzionarie introdotte dal modello cinese √® l‚Äôuso del Reinforcement Learning (RL) senza Supervised Fine-Tuning (SFT), che ha permesso di sviluppare capacit√† di ragionamento avanzate senza la necessit√† di una fase iniziale di addestramento supervisionato. Il modello sfrutta un algoritmo di ottimizzazione durante il processo di RL, chiamato Group Relative Policy Optimization, riducendo il costo computazionale e migliorando la stabilit√† dell‚Äôaddestramento. Un‚Äôaltra innovazione chiave √® l‚Äôadozione di un reward modeling basato su regole, che valuta la qualit√† delle risposte tramite criteri predefiniti, evitando l‚Äôuso di modelli di ricompensa neurali che possono essere soggetti a manipolazione (reward hacking).
Inoltre, DeepSeek-R1 ha migliorato la leggibilit√† e la coerenza delle risposte grazie ai cold start data, un insieme limitato di dati utilizzati prima dell‚ÄôRL per affinare il comportamento del modello. Un altro elemento distintivo √® la distillazione delle capacit√† di ragionamento, che consente di trasferire competenze da modelli di grandi dimensioni a versioni pi√π piccole, mantenendo prestazioni elevate con un minor costo computazionale. Infine, DeepSeek-R1 adotta un approccio multi-stage training, combinando pi√π fasi di RL e SFT per ottimizzare sia le capacit√† di ragionamento che quelle generali del modello.
Questa combinazione di innovazioni ha permesso a DeepSeek-R1 di distinguersi come un modello all'avanguardia nel campo dell'intelligenza artificiale, pur mantenendo un'impronta computazionale relativamente contenuta rispetto ai concorrenti statunitensi.
Elaborazione del linguaggio naturale in ambito educativo
L'elaborazione del linguaggio naturale, o NLP (Natural Language Processing), trova applicazione anche nell‚Äôambito della didattica e della scuola. Si rivela utile per riepilogare grandi volumi di testo, per tradurli da una lingua a un'altra, e rivela la sua efficacia anche nel rispondere a comandi digitati o parlati.
In ambito scolastico, i fruitori di tali nuove tecnologie comprendono gli studenti e l‚Äôintero sistema scuola. Non √® solo il modo di apprendere che subisce modifiche con l‚Äôavvento di nuovi software di intelligenza artificiale (IA) ma anche il modo di insegnare, di progettare l‚Äôinsegnamento e di viverlo all‚Äôesterno dell‚Äôorario scolastico e quindi saranno i docenti, i dirigenti, il personale amministrativo ma anche le famiglie a vedersi protagonisti in questa rivoluzione.
Tuttavia, affinch√© l‚Äôintegrazione dell‚ÄôIA nell‚Äôambiente educativo sia consapevole e porti a risultati efficaci √® bene che sia verificabile il principio di ‚Äúexplainability‚Äù, ovvero di spiegabilit√†, che si riferisce alla possibilit√† di comprendere il processo per cui un input dato allo strumento produce un determinato output. In questo senso √® doveroso sviluppare una cultura dell'intelligenza artificiale.
Per esempio, la prompt engineering √® una disciplina che si √® sviluppata con la volont√† di costruire i giusti prompt da fornire agli strumenti cos√¨ da permetterne un funzionamento preciso e coerente rispetto alla richiesta avanzata dall'utente.
Valutazione dei compiti
Tra le importanti modifiche apportate dall‚ÄôIA nel sistema scolastico si pu√≤ sicuramente annoverare l'introduzione dei punteggi automatici che vedono l‚Äôelaborazione del linguaggio naturale come supporto fondamentale per il loro corretto funzionamento.
I sistemi di punteggio automatico ad oggi sono strumenti di grande utilit√† poich√© offrono una valutazione tempestiva, precisa ed efficace delle competenze dello studente nella risoluzione di un dato problema, promuovendo un ambiente di apprendimento adattivo grazie alla possibilit√† di ricevere un feedback immediato che dia agli studenti modo di riconoscere e correggere le incomprensioni affinch√© possano padroneggiare meglio l'argomento in questione.
L'uso dell'IA nella valutazione offre diversi vantaggi, tra i quali:
La velocit√†: gli algoritmi di IA possono correggere un gran numero di compiti in una frazione del tempo necessario a un essere umano, consentendo agli insegnanti di dedicare pi√π tempo alla didattica e al supporto personalizzato degli studenti.
Feedback personalizzati: le tecnologie di IA possono fornire feedback dettagliati e personalizzati in tempo reale, aiutando gli studenti a comprendere meglio i propri errori e a migliorare le proprie competenze. Ad esempio, piattaforme come Grammarly utilizzano l'IA per suggerire correzioni grammaticali e stilistiche nei testi scritti dagli studenti.
Analisi predittiva: l'IA pu√≤ analizzare i dati di apprendimento degli studenti per identificare pattern e tendenze, permettendo agli educatori di intervenire tempestivamente con strategie mirate per supportare gli studenti in difficolt√†. Questa analisi pu√≤ anche prevedere le prestazioni future degli studenti, aiutando a personalizzare i percorsi di apprendimento.
Questo tipo di interazione immediata √® particolarmente utile per l'apprendimento delle lingue straniere e delle materie scientifiche.
Strumenti per la correzione e la valutazione dei compiti
Negli Stati Uniti, ad esempio, gli insegnanti delle scuole medie stanno iniziando a utilizzare un nuovo strumento di correzione basato su ChatGPT chiamato Writable.
Acquisito da Houghton Mifflin, Writable √® progettato per semplificare il processo di correzione e risparmiare tempo agli insegnanti. Gli insegnanti possono inviare i temi degli studenti per l'analisi, ricevendo commenti e osservazioni generati dall'IA, che vengono poi revisionati dai docenti prima di essere consegnati agli studenti.
Gli strumenti di elaborazione del linguaggio naturale hanno il potenziale di trasformare radicalmente la valutazione dei compiti, rendendola pi√π efficiente e informativa. Tuttavia, per sfruttare appieno questi vantaggi, √® necessario affrontare le sfide con un approccio ponderato e inclusivo. Con un'implementazione attenta e responsabile, l'IA pu√≤ diventare uno strumento prezioso per migliorare l'esperienza educativa e promuovere l'apprendimento personalizzato.
Mediatori Visivi e Mappe Concettuali
I mediatori visivi, come le mappe concettuali, sono strumenti diagrammatici e visivi utilizzati per rappresentare e organizzare idee in modo non lineare. Questi strumenti sono stati sviluppati negli anni '70 presso la Cornell University, con l'obiettivo di illustrare le comprensioni concettuali dei bambini in ambito scientifico. Le mappe concettuali consentono di visualizzare fenomeni complessi, facilitando la creazione di nuove connessioni e la costruzione di conoscenze.
Utilizzate ampiamente nelle discipline del design e nelle arti visive, le mappe concettuali servono come metodo di brainstorming per documentare idee e processi di progettazione fin dalle fasi iniziali. La loro utilit√† si estende anche alla ricerca qualitativa, dove aiutano i ricercatori a tracciare e analizzare le relazioni emergenti tra i dati. Mediante schizzi fatti a mano o strumenti digitali, le mappe concettuali permettono di visualizzare le inter-relazioni tra concetti, rendendo il processo di pensiero pi√π trasparente e facilitando l'elaborazione di analisi e sintesi dei dati complessi. Tra i principali mediatori visivi ci sono le mappe concettuali e mentali, i diagrammi di flusso, i grafici e le tabelle, i diagrammi di Venn, gli storyboard, le infografiche, i diagrammi a ragno e a cascata, e i diagrammi SWOT. Questi strumenti permettono di organizzare, comprendere e comunicare dati e concetti in modo efficace, migliorando l'apprendimento.
L'intelligenza artificiale sta rivoluzionando la generazione di mappe concettuali attraverso mediatori visivi avanzati. Utilizzando algoritmi di machine learning e reti neurali, le soluzioni basate su IA sono in grado di analizzare grandi quantit√† di dati testuali e convertirli in mappe concettuali visive in modo automatico. Questi strumenti possono identificare relazioni chiave e concetti emergenti, offrendo rappresentazioni visive che facilitano la comprensione e l'analisi delle informazioni complesse. Questa tecnologia trova applicazione in vari campi, tra cui l'istruzione, la ricerca accademica e il business, contribuendo a semplificare il processo di apprendimento e decision-making. La capacit√† dell'IA di aggiornare e adattare continuamente le mappe concettuali in base a nuovi dati assicura che queste rappresentazioni rimangano accurate e rilevanti nel tempo.
Strumenti per la generazione di mappe concettuali
Esistono diverse piattaforme che utilizzano l'elaborazione del linguaggio naturale per generare mappe concettuali. Alcune delle pi√π note includono:
MindMeister: offre funzionalit√† di mappatura mentale basate su cloud, con l'integrazione di IA per suggerire connessioni e organizzare le idee in modo intuitivo.
Lucidchart: sebbene sia principalmente uno strumento di diagrammazione, Lucidchart utilizza l'IA per ottimizzare la creazione di diagrammi di flusso e mappe concettuali, facilitando la visualizzazione delle idee e delle relazioni tra concetti.
Coggle: una piattaforma di mappatura mentale che sfrutta l'IA per migliorare la creazione e l'organizzazione delle mappe concettuali, rendendo pi√π facile la collaborazione in tempo reale.
XMind: utilizza l'intelligenza artificiale per assistere nella creazione di mappe mentali e concettuali, con suggerimenti intelligenti e layout automatici per migliorare la chiarezza e l'efficacia delle rappresentazioni visive.
Ayoa: combina funzionalit√† di mappatura mentale integrando l'IA per suggerire collegamenti tra concetti e per aiutare nella pianificazione e organizzazione delle idee.
Miro: una lavagna collaborativa online che utilizza l'IA per supportare la creazione di mappe concettuali e altri tipi di diagrammi, facilitando la collaborazione e l'ideazione visiva.
Riconoscimento vocale e dettatura Speech to Text (STT)
Il riconoscimento vocale (Speech to Text) √® in grado di convertire in modo affidabile dati vocali in dati di testo. Viene utilizzato da qualsiasi software o applicazione che necessiti di rispondere a domande parlate o comandi vocali in generale. Il riconoscimento vocale non √® un compito facile per la macchina. L‚Äôostacolo principale √® dovuto alle specificit√† del linguaggio parlato umano: la velocit√† del parlato, le diverse sfumature di accenti e intonazioni, la possibilit√† di inserire enfasi e toni particolari, la difficile comprensione del volume della voce e l'eventuale grammatica scorretta. Per svolgere questo compito l‚ÄôIA specializzata nell‚ÄôNLP utilizza diverse tecniche che gli consentono di superare tali ostacoli e rispondere con grande precisione linguistica e comunicativa, adattandosi al destinatario.
L'utilizzo dei software Speech to Text in ambito scolastico √® consolidato, si rivelano funzionali per prendere appunti velocemente, scrivere e-mail senza dover digitarne il testo, trascrivere riunioni o lunghe conversazioni. Oltre a questi usi legati alla produttivit√†, si tratta di software inclusivi che aiutano anche nella compensazione di numerose disabilit√†.
Esempi di assistenti vocali
Alexa: progettato da Amazon √® tra i pi√π acquistati dagli utenti, si attiva attraverso la pronuncia della parola ‚ÄúAlexa‚Äù seguita dalla richiesta o dal comando.
Siri: disponibile su tutti i dispositivi Apple, viene attivato dall‚Äôutente mediante la keyword ‚ÄúHey, Siri‚Äù seguita dalle richieste.
Microsoft Cortana: supportato da tutti i sistemi operativi Windows 10 per PC, Windows Phone e Xbox One, √® attivabile tramite la parola chiave ‚ÄúHey, Cortana‚Äù seguita dalla richiesta.
Google Assistant: assistente vocale Android integrato in tutti i dispositivi speaker Google.
Esempi di software Speech to Text
Google Docs Voice Typing: √® un software Google gratuito integrato in Google Docs
Speech to Text di Microsoft Azure: trascrive rapidamente e accuratamente il parlato in testo in pi√π di 100 lingue diverse.
Sintesi vocale e software Text to Speech (TTS)
La sintesi vocale √® una tecnica di riproduzione artificiale della voce, realizzata tramite sintetizzatori vocali che utilizzano software Text to Speech (TTS), trasformando il testo scritto in parlato. √à ampiamente utilizzata in ambito scolastico e formativo, in modo particolare per supportare studenti con diagnosi di Disturbi Specifici dell'Apprendimento (DSA).
La sintesi vocale √® impiegata come strumento compensativo per gli studenti con dislessia, permettendo di compensare le difficolt√† di lettura. Grazie a questo strumento, lo studente pu√≤ evitare di affrontare compiti di lettura che richiederebbero per lui uno sforzo significativo, senza per√≤ ottenere alcun beneficio nell'apprendimento.
L'utilizzo della sintesi vocale √® riconosciuto dalla Legge italiana n. 170 dell‚Äô8 ottobre 2010 come strumento compensativo per gli alunni con DSA che in quanto tale "trasforma un compito di lettura in un compito di ascolto", facilitando cos√¨ il processo di apprendimento per questi studenti.
Traduzione linguistica
I servizi di elaborazione del linguaggio naturale hanno consentito grandi passi nel miglioramento delle prestazioni dei software di traduzione linguistica. La traduzione automatica √® il processo di utilizzo dell‚Äôintelligenza artificiale per tradurre automaticamente il contenuto da una lingua all‚Äôaltra senza l‚Äôintervento umano. Grazie alle tecniche di NLP, i traduttori online sono in grado di tradurre in modo sempre pi√π accurato, fornendo risultati grammaticalmente corretti. Questi software sono dunque sempre pi√π utilizzati anche per fini didattici e di apprendimento.
Alcuni esempi dei software di traduzione linguistica maggiormente utilizzati per la didattica
Grammarly: inserendo una frase scritta in inglese, questa app non solo individua errori grammaticali o di sintassi. Infatti, analizzando la frase inserita, offre suggerimenti su come migliorare lo stile e le espressioni.
WordFisher: √® uno strumento avanzato per traduttori che lavorano con i documenti di Microsoft Word. WordFisher √® un programma che necessita di Word per lavorare: le sue funzioni vengono infatti installate nel menu della barra degli strumenti di Word.
Google Traduttore: √® un servizio di traduzione automatica multilingue sviluppato da Google LLC. √à utilizzabile sul Web o tramite l'app Google Traduttore. Consente di tradurre testi, scrittura a mano libera, foto e contenuti vocali in oltre 100 lingue.
Reverso: traduce documenti di tutti i tipi, in oltre 25 lingue, lasciando la formattazione originaria. Trova sinonimi in tempo rapido, grazie all‚Äôuso integrato di pi√π dizionari, √® integrato con un correttore automatico e possiede una versione gratuita.
Wordreference: √® uno dei dizionari online pi√π utilizzati, consente la traduzione in circa 20 lingue, ha un coniugatore di verbi, un forum, prevede alcuni dizionari monolingue ed √® completamente gratuito.
Assistenti conversazionali
I motori di ricerca tradizionali come Google ad esempio, identificano i siti contenenti le parole ricercate sulla base del loro algoritmo e sta poi all‚Äôutente svolgere un'operazione di selezione e verifica dell'attendibilit√† delle fonti. Nel caso degli ‚Äúassistenti conversazionali‚Äù invece, basta formulare correttamente la domanda e il testo verr√† generato in maniera automatica dal software.
Gli assistenti conversazionali utilizzano grandi volumi di dati, il machine learning e l'elaborazione del linguaggio naturale per riconoscere input vocali e testuali e tradurne i significati in varie lingue. Attraverso ‚Äúprompt‚Äù ben costruiti questi software sono in grado di variare i registri linguistici (giornalistico, scientifico, narrativo ecc.) e le et√† dei destinatari dei testi, i processi NLP fluiscono continuamente in un ciclo di feedback costante allo scopo di migliorare sempre di pi√π gli algoritmi di IA.
Tra gli assistenti conversazionali maggiormente utilizzati in ambito scolastico abbiamo ChatGPT. Questa piattaforma deve il suo continuo progresso all‚Äôuso di quantit√† sempre crescenti di dati per addestrare il suo numero esponenzialmente crescente di parametri che gli permette di proseguire la conversazione migliorando e affinando progressivamente l‚Äôoutput finale.
Criticit√† dell‚Äôelaborazione del linguaggio naturale
In generale, l'impiego dell'Intelligenza Artificiale e delle sue declinazioni sia in ambito educativo che generale comporta delle criticit√† condivise, tra cui:
L'enorme mole di dati e informazioni che vengono processati per elaborare i testi e i linguaggi: sono richieste eccessive spese monetarie sia per l‚Äôacquisto che per il mantenimento e supporto delle IA, le cui stesse sono di propriet√† di aziende tecnologiche e quindi disponibili soltanto nei Paesi del mondo pi√π ricchi, principalmente America, Cina ed Europa, creando cos√¨ grande disparit√† con i Paesi pi√π poveri, specialmente Sud America e Africa, generando una situazione di ‚Äúdata poverty‚Äù.
Limitazioni contestuali: i modelli delle IA basate sull‚Äôelaborazione del linguaggio naturale, possono presentare delle limitazioni del linguaggio basate sul contesto culturale di un Paese di riferimento. Questo pu√≤ portare a fraintendimenti, oltre che analisi scorrette dei testi.
Ambiguit√† e polisemie: molte parole e frasi possono avere molteplici significati, creando difficolt√† nelle IA basate sull‚ÄôNLP di comprensione nell‚Äôintenzione e utilizzo, oltre che nell‚Äôintenzione in un determinato contesto. Questo pu√≤ portare ad una inaccuratezza dei dati analizzati.
Diversit√† dei linguaggi e idiomi: la vastissima diversit√† dei linguaggi e delle varianti regionali quali dialetti, slang o colloquialismi risultano difficili da essere compresi e analizzati per i modelli delle IA basate sull‚ÄôNLP.
Mancanza di ragionamento implicito: al contrario degli umani, che spesso basano la comunicazione anche attraverso l‚Äôutilizzo di codici linguistici impliciti basati sul contesto, cultura generale e buon senso, le macchine IA sono completamente prive di questo aspetto, ostacolando la loro capacit√† di analisi di un testo dove sono presenti pi√π sfumature e informazioni implicite, come un linguaggio di tipo sarcastico.
Emoticon e caratteri speciali: con l‚Äôavvento e utilizzo dei media digitali, le emoticon e i caratteri speciali vengono sempre pi√π utilizzati in contesti online, creando per√≤ difficolt√† nelle IA nel riuscire a riconoscerli e a utilizzarli in base al contesto di riferimento.
Precisione dei dati inseriti: per garantire risultati eccellenti, √® importante assicurarsi che chi si occupa dei training di allenamento delle macchine IA inserisca dei dati quanto pi√π accurati possibili. In caso contrario, l‚Äôinserimento di dati incompleti o erronei potrebbero comportare risultati distorti, rafforzando pregiudizi sociali e stereotipi.
Isabella Chiari, Introduzione alla linguistica computazionale, Bari, Laterza, 2007, ISBN 978-88-420-8209-5.
Intelligenza artificiale
Linguistica computazionale
Trasformatore (informatica)
Rete neurale artificiale
Analisi del sentiment
Wikimedia Commons contiene immagini o altri file sull'elaborazione del linguaggio naturale
Elaborazione del linguaggio naturale, su Vocabolario Treccani, Istituto dell'Enciclopedia Italiana, 2023.
(EN) Tara Ramanathan, natural language processing, su Enciclopedia Britannica, Encyclop√¶dia Britannica, Inc.
(EN) Opere riguardanti Natural language processing (Computer science) / Natural Language Processing, su Open Library, Internet Archive.